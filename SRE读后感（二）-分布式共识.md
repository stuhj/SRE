最近一段时间阅读了SRE这本书中有关分布式共识的内容，并且依照raft-extends这篇论文实现了mit6.824中lab2，实现一个简单的raft。

本文主要分成以下三部分
- 分布式共识
- Raft算法与实验
- 站在sre的视角看分布式共识

### **什么是分布式共识**

我们通常使用分布式系统来提高服务的可用性。与此同时，我们需要保证多个进程有着相同的状态，以便在故障切换时保证持续的、正确的对外服务能力。

解决分布式共识/一致性问题已经有了很完善的做法，最经典的是Paxos，但是Paxos非常复杂，在工程中通常使用raft，zab协议来代替Paxos解决分布式共识问题。

### **Raft**

Raft是一个解决分布式共识的算法。
- 我们把每一个进程抽象成一个节点。
- 每个节点有三个状态：Follower,Candidate,Leader

**OverView of Raft**

**Leader Election**

1. 所有的节点都处于Follower状态。
2. 如果一个节点在一段时间内，没有感知到来自Leader的心跳，那么这个节点就进入Candidate状态。
3. 进入Candidate状态的节点会向其它节点申请votes。
4. 所有节点会回复它们的选票，若Candidate获得了majority（超过半数）的选票，那么它进入Leader状态。

**Log Replication**

在选出Leader后，客户端对系统状态的所有改变都要通过Leader节点。

1. 收到客户端的改变系统状态的请求时，都先以追加写的形式将操作写入Leader节点日志。此时，该改动处于uncommitted状态，所以还没有真正触发改变进程状态的操作。
2. Leader将此次操作日志复制给Follower节点。
3. Follower节点在更新日志后回复Leader节点。
4. 当Leader节点收到了majority（超过半数）的节点回复后，执行客户端请求的操作，改变进程状态。并通知Follower执行操作。

### **Raft实现细节：**

**选举超时时间T1：**

若Follower在等待T1时间后，仍然没有收到来自Leader的心跳，则进入Candidate状态。

每个节点的超时时间是一个随机值[150ms, 300ms]。随机值是为了解决多个节点始终同时进入candidate状态导致选票被瓜分，长时间得不到选举结果的问题。

**投票：**

Candidate节点会先投自己一票，Follower在收到Candidate的vote请求后，若在本轮选举（term）中还没有投出票，则为该Candidate投票，并重置选举超时。

**两个Follower同时进入候选状态并获得了相同的选票：**

两个Candidate重复选举过程，因为超时时间是随机的，因此再次获得相同选票的概率很低。


**关于网络产生分区（network partition）时的共识:**

在满足以下条件时：

1. 若网络出现分区。
2. 且在选举超时时间内未回复。
3. 存在某一分区不包含原Leader节点，且分区内包含较多节点（大于N/2）。

此时在3中描述的分区中会出现新的Leader节点。

当网络恢复之后，所有节点会认同选举轮数（Election Term）较大的Leader节点为最终Leader, （e.g.原Leader所在分区的选举轮数会小于新Leader所在的分区）。轮数较低分区内的节点需要回滚状态，并同步日志，达到分布式系统状态一致。

注：当上述三个条件满足时，客户端向原Leader发出请求时，原Leader由于得不到majority节点的回复，是无法完成客户端请求的。



### **实验mit 6.824 lab2**
**实验代码Git:** https://github.com/stuhj/mit6.824/blob/master/src/raft

**实验结果:**

![image](http://note.youdao.com/yws/public/resource/bc95c9115d7a3435cbee9d0e91d97b49/xmlnote/14D20CAF2AF9436A8168FD7656684414/2137)
当前做到lab2的A,B两部分

Test（2A）：Leader选举，Passed（2/2）

Test（2B）：一致性检验，Passed（6/7）

未通过的场景：该场景就是上述网络产生分区后的共识问题。

5个raft节点，产生网络分区后，分区节点比为2:3。原Leader1在2个节点的分区中，因此无法提交客户端的请求。在3个节点的分区中产生新的Leader2，可以提交客户端请求。

分别向leader1和leader2发送请求，导致leader1和leader2的日志不不同步。
在网络分区恢复后，检查一致性。

一致性解决办法：在网络故障恢复后，leader1和leader2都会接到来自对方的心跳包，这时：

1. 任期(term)低的节点自动降级为follower。

![image](http://note.youdao.com/yws/public/resource/bc95c9115d7a3435cbee9d0e91d97b49/xmlnote/98E4D51E4E064EBC98614A97CE214563/2139)

2. 通过比对log找到最后同步的log_id（必须index和term都一致）。

![image](http://note.youdao.com/yws/public/resource/bc95c9115d7a3435cbee9d0e91d97b49/xmlnote/92BA36B7EE7C4B8B8F983779CE4398D4/2141)

3. leader在得到rpc返回值后更新nextLogIndex[follower]。

![image](http://note.youdao.com/yws/public/resource/bc95c9115d7a3435cbee9d0e91d97b49/xmlnote/78F0D90128064412A4CC8BD3FC86865C/2143)

4. 在下一次发送心跳时，降级的follower会同步正确的日志。

![image](http://note.youdao.com/yws/public/resource/bc95c9115d7a3435cbee9d0e91d97b49/xmlnote/94D422A3BD234407A7C8AFBB4AD64031/2145)

按照这种方法，该Test没有通过.....


### **站在SRE角度看分布式共识**
**分布式公式系统的性能问题：**

没有性能最优的分布式共识算法。性能取决于与系统负载有关的多个因素。

**系统负载：**
1. 吞吐量
2. 请求类型
3. 读请求的一致性要求
4. 数据大小和请求大小
5. 部署策略
6. 仲裁方法，进程的分布情况
7. 系统是否使用分片、批处理、流水线技术。

##### 读操作
强一致性：保证读取的数据一定是最新的。
1. 读之前做一次读一致性检查。
2. 保证从最新副本处读取（比如raft中的Leader）。
3. 使用法定租约。

弱一致性：不同客户端可以从不同的节点上读取数据，这些数据可能会有一些不一致（新旧不同，但至少不是错误的）。

法定租约：保证部分节点的数据在租约期间是强一致的，读数据时，可以在任意有租约的节点上读取。

注：由于需要保证多个节点间的强一致性，该系统的写性能会有一定程度的下降。租约机制适用于大量读操作的系统中。

##### 写操作
写操作性能与RTT、写磁盘时间相关。

**RTT**

1. 源地址、目的地址之间的距离：通常跨域连接，所以需要考虑。
2. 网络状况：可能会有丢包重发的问题，外网通常不可控？
3. 协议：考虑Tcp/Ip三次握手，慢起动。

> 对于共识组中的进程：保持一个长连接心跳可解决。
> 
> 对于客户端与共识组：服务端维护大量客户端连接不现实，可以考虑区域代理连接池。

4. 批处理：客户端积攒一部分请求后统一发送给给服务端，增加吞吐量。
5. 流水线：书中没有给出具体的例子，根据分析，应该是将一个大流程分解，比如：
> 之前Leader发送log给其它followers再得到它们的响应是一个原子过程；
>
> 那么我们可以使用两个线程处理这个过程，线程1负责给followers发送log，线程2负责接收来自follower的响应，这样就不会出现leader由于等待响应使客户端请求排队的状况了。
> （这种流水线模式还可以使用RPC通信么？）

**写磁盘**
1. 合并日志，减少磁盘寻道时间。
2. 善用缓存。

### 分布式共识系统的部署

**副本数量：**

拜占庭错误：3f+1个副本可以容忍f个副本的错误。

非拜占庭错误：2f+1个副本可以容忍f个副本的错误。

**副本位置：**

需要考虑故障域。

**容量规划**

1. 副本数量增加可能会导致可用性下降：5个副本，允许2个（40%）副本故障。6个副本同样只允许2个（33%）副本故障。
2. 增加副本会增加Leader的负载。

**负载均衡**
1. 客户端与哪一个副本通信：若优先与距离近的副本通信，有着良好的rtt，若某一个区域内客户端数量庞大，会导致副本过载，之后同时迁移到第二个副本，很容易产生连锁效应，再次吞没副本。

（我在实习的时候rpc客户端默认连接到lvs上，由lvs做负载均衡）
2. 领头人往往占用更多的带宽资源：需要考虑领头人进程正常工作时的带宽容量，同时也需要考虑领头人进程宕掉后，负载转移策略，目的机器是否资源可用。
3. 多个Leader在同一个数据中心中，若该数据中心故障，leader的迁移会造成系统抖动。因此迁移算法需要考虑这方面内容。

**仲裁组**

![image](http://note.youdao.com/yws/public/resource/bc95c9115d7a3435cbee9d0e91d97b49/xmlnote/98CB8E177F244917AAD16CF6423BC314/2147)

这个例子很有趣：因为跨洋RTT很高，因此该仲裁组中有三个节点设置在美洲，两个设置在欧洲。只要美洲的三个节点都正常工作，那么仲裁过程不需要等待欧洲的两个节点的响应，提高性能。

明显，东海岸的节点故障后，后大大降低性能。于是有了下面的架构
![image](http://note.youdao.com/yws/public/resource/bc95c9115d7a3435cbee9d0e91d97b49/xmlnote/FBDBB1A455BF4B8C9679D668B8C9FC5D/2149)

分层仲裁。只有东海岸三个节点中的两个以上失效时，才会影响性能。




